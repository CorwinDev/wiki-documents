"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[11179],{15680:(e,t,n)=>{n.d(t,{xA:()=>d,yg:()=>g});var a=n(96540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(n),m=o,g=c["".concat(l,".").concat(m)]||c[m]||u[m]||i;return n?a.createElement(g,r(r({ref:t},d),{},{components:n})):a.createElement(g,r({ref:t},d))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},14999:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(9668),o=(n(96540),n(15680));const i={description:"This wiki provides a tutorial on how to run a visual language model with speech interaction on the reComputer AGX Orin 64G.",title:"Run VLM with speech interaction",keywords:["Multimodal","NanoVLM","TTS","STT"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/speech_vlm",last_update:{date:"08/23/2024",author:"YaoHui Zhu"}},r="How to run a multimodal visual language model with speech interaction on a reComputer Nvidia Jetson.",s={unversionedId:"Edge/NVIDIA_Jetson/Application/Multimodal_AI/Speech_vlm",id:"Edge/NVIDIA_Jetson/Application/Multimodal_AI/Speech_vlm",title:"Run VLM with speech interaction",description:"This wiki provides a tutorial on how to run a visual language model with speech interaction on the reComputer AGX Orin 64G.",source:"@site/docs/Edge/NVIDIA_Jetson/Application/Multimodal_AI/Speech_vlm.md",sourceDirName:"Edge/NVIDIA_Jetson/Application/Multimodal_AI",slug:"/speech_vlm",permalink:"/speech_vlm",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Edge/NVIDIA_Jetson/Application/Multimodal_AI/Speech_vlm.md",tags:[],version:"current",lastUpdatedBy:"YaoHui Zhu",lastUpdatedAt:1724371200,formattedLastUpdatedAt:"Aug 23, 2024",frontMatter:{description:"This wiki provides a tutorial on how to run a visual language model with speech interaction on the reComputer AGX Orin 64G.",title:"Run VLM with speech interaction",keywords:["Multimodal","NanoVLM","TTS","STT"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/speech_vlm",last_update:{date:"08/23/2024",author:"YaoHui Zhu"}},sidebar:"ProductSidebar",previous:{title:"Local AI Assistant",permalink:"/local_ai_ssistant"},next:{title:"Install ROS1",permalink:"/installing_ros1"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"VLM (Visual Language Model) Introduction",id:"vlm-visual-language-model-introduction",level:3},{value:"SenseVoice Introduction",id:"sensevoice-introduction",level:3},{value:"TTS (Text-to-Speech) Introduction",id:"tts-text-to-speech-introduction",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Installation",id:"installation",level:2},{value:"Initialize system environment",id:"initialize-system-environment",level:3},{value:"Install VLM",id:"install-vlm",level:3},{value:"Install Pytorch Torchaudio",id:"install-pytorch-torchaudio",level:3},{value:"Install Speech_vlm (Based on SenseVoice)",id:"install-speech_vlm-based-on-sensevoice",level:3},{value:"Install TTS (Based on Coqui-ai)",id:"install-tts-based-on-coqui-ai",level:3},{value:"Usage",id:"usage",level:2},{value:"Demonstration",id:"demonstration",level:3},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],d={toc:p},c="wrapper";function u(e){let{components:t,...n}=e;return(0,o.yg)(c,(0,a.A)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"how-to-run-a-multimodal-visual-language-model-with-speech-interaction-on-a-recomputer-nvidia-jetson"},"How to run a multimodal visual language model with speech interaction on a reComputer Nvidia Jetson."),(0,o.yg)("h2",{id:"introduction"},"Introduction"),(0,o.yg)("p",null,"This guide provides a detailed explanation on how to run a multimodal Visual Language Model (VLM) with speech interaction on a reComputer Nvidia Jetson device. The model leverages the powerful computational capabilities of the official Nvidia Jetson platform, combined with Alibaba's open-source speech-to-text model SenseVoice and coqui-ai's text-to-speech (TTS) model, to perform complex multimodal tasks. By following this guide, you will be able to successfully install and operate this system, enabling it with both visual recognition and speech interaction capabilities, thereby offering smarter solutions for your projects."),(0,o.yg)("h3",{id:"vlm-visual-language-model-introduction"},(0,o.yg)("a",{parentName:"h3",href:"https://docs.nvidia.com/jetson/jps/inference-services/vlm.html"},"VLM (Visual Language Model) Introduction")),(0,o.yg)("p",null,"The Visual Language Model (VLM) is a multimodal model optimized for the Nvidia Jetson platform. It combines visual and language processing to handle complex tasks, such as object recognition and generating descriptive language. VLM is applicable in fields like autonomous driving, intelligent surveillance, and smart homes, offering intelligent and intuitive solutions."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/vlmgif.gif"})),(0,o.yg)("h3",{id:"sensevoice-introduction"},(0,o.yg)("a",{parentName:"h3",href:"https://github.com/FunAudioLLM/SenseVoice/tree/main"},"SenseVoice Introduction")),(0,o.yg)("p",null,"SenseVoice is an open-source model focused on high-accuracy multilingual speech recognition, speech emotion recognition, and audio event detection. Trained on over 400,000 hours of data, it supports 50+ languages and outperforms the Whisper model. The SenseVoice-Small model delivers ultra-low latency, processing 10 seconds of audio in just 70ms. It also provides convenient finetuning and supports deployment in multiple languages, including Python, C++, HTML, Java, and C#."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/sensevoice2.png"})),(0,o.yg)("h3",{id:"tts-text-to-speech-introduction"},(0,o.yg)("a",{parentName:"h3",href:"https://github.com/coqui-ai/TTS"},"TTS (Text-to-Speech) Introduction")),(0,o.yg)("p",null,"The TTS model is a high-performance deep learning model for text-to-speech tasks. It includes various models like Tacotron2 and vocoders such as MelGAN and WaveRNN. The TTS model supports multi-speaker TTS, efficient training, and offers tools for dataset curation and model testing. Its modular codebase allows easy implementation of new features."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png"})),(0,o.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"reComputer Jetson AGX Orin 64G or reComputer Jetson J4012 16G device with more than 16GB of memory."),(0,o.yg)("li",{parentName:"ul"},"USB driver-free speaker microphone"),(0,o.yg)("li",{parentName:"ul"},"An IP camera that can output an RTSP stream address. We have also included instructions on ",(0,o.yg)("a",{parentName:"li",href:"/getting_started_with_nvstreamer"},"how to use the NVIDIA Nvstreamer")," tool to convert local videos into RTSP streams.")),(0,o.yg)("admonition",{type:"note"},(0,o.yg)("p",{parentName:"admonition"},"We have already tested the feasibility of this wiki on reComputer ",(0,o.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html"},"Orin NX 16GB")," and ",(0,o.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/NVIDIArJetson-AGX-Orintm-64GB-Developer-Kit-p-5641.html"},"AGX Orin 64GB")," Developer Kit.")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer-Jetson/Llama-Factory/agx_orin.png"})),(0,o.yg)("div",{class:"get_one_now_container",style:{textAlign:"center"}},(0,o.yg)("a",{class:"get_one_now_item",href:"https://www.seeedstudio.com/AGX-Orin-32GB-H01-Kit-p-5569.html?queryID=a07376a957f072a4f755e1832fa0e544&objectID=5569&indexName=bazaar_retailer_products"},(0,o.yg)("strong",null,(0,o.yg)("span",null,(0,o.yg)("font",{color:"FFFFFF",size:"4"}," Get One Now \ud83d\uddb1\ufe0f"))))),(0,o.yg)("h2",{id:"installation"},"Installation"),(0,o.yg)("h3",{id:"initialize-system-environment"},"Initialize system environment"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"After installing the initial system with JP6, you need to check the installation of ",(0,o.yg)("inlineCode",{parentName:"p"},"CUDA")," and other libraries. You can verify and install them by running ",(0,o.yg)("inlineCode",{parentName:"p"},"sudo apt-get install nvidia-jetpack"),".")),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Install ",(0,o.yg)("inlineCode",{parentName:"p"},"python3-pip"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"jtop"),", and ",(0,o.yg)("inlineCode",{parentName:"p"},"docker-ce"),".")),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Install the necessary dependencies by running the following commands:"),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"sudo apt-get install libportaudio2 libportaudiocpp0 portaudio19-dev\nsudo pip3 install pyaudio playsound subprocess wave keyboard\nsudo pip3 --upgrade setuptools\nsudo pip3 install sudachipy==0.5.2\n"))),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Check that the audio input and output, as well as the USB speaker microphone, are functioning properly and that the network connection is stable."))),(0,o.yg)("h3",{id:"install-vlm"},"Install VLM"),(0,o.yg)("p",null,"The core functionality of this project is the visual language model (VLM). We have provided a guide on ",(0,o.yg)("a",{parentName:"p",href:"/run_vlm_on_recomputer"},"how to use the VLM on the reComputer Nvidia Jetson"),". Please refer to this link for installation and usage instructions. Make sure you fully understand how to perform inference using text descriptions in the VLM before proceeding with the following steps."),(0,o.yg)("h3",{id:"install-pytorch-torchaudio"},"Install Pytorch Torchaudio"),(0,o.yg)("p",null,"We have provided an Nvidia Jetson AI course for beginners, which includes instructions on ",(0,o.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Projects/reComputer-Jetson-for-Beginners/blob/main/3-Basic-Tools-and-Getting-Started/3.3-Pytorch-and-Tensorflow/README.md"},"how to install PyTorch, Torchaudio, and Torchvision"),". Please download and install these packages according to your system environment."),(0,o.yg)("h3",{id:"install-speech_vlm-based-on-sensevoice"},"Install Speech_vlm (Based on SenseVoice)"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Clone Speech_vlm packages:",(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/\ngit clone https://github.com/ZhuYaoHui1998/speech_vlm.git\n"))),(0,o.yg)("li",{parentName:"ol"},"Install Speech_vlm environment:",(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/speech_vlm\nsudo pip3 install -r requement.txt\n")))),(0,o.yg)("h3",{id:"install-tts-based-on-coqui-ai"},"Install TTS (Based on Coqui-ai)"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/speech_vlm/TTS\nsudo pip3 install .[all]\n")),(0,o.yg)("h2",{id:"usage"},"Usage"),(0,o.yg)("p",null,"The structure of the speech_vlm repository is as follows:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"speech_vlm/\n\u251c\u2500\u2500 /TTS   # Coqui-ai TTS program\n\u251c\u2500\u2500 config # VLM config\n\u251c\u2500\u2500 README.md    #Project Introduction\n\u251c\u2500\u2500 requirements.txt   #SenseVoice required environment libraries\n\u251c\u2500\u2500 compose.yaml   #VLM Docker Compose startup file\n\u251c\u2500\u2500 delete_id.sh     #Delete camera ID script\n\u251c\u2500\u2500 example_1.wav     #Audio feedback sound tone template (replaceable)\n\u251c\u2500\u2500 model.py     #SenseVoice main program\n\u251c\u2500\u2500 set_alerts.sh     #Set up camera alerts\n\u251c\u2500\u2500 set_describe.sh     #Text input to have the VLM describe the current scene\n\u251c\u2500\u2500 set_streamer_id.sh  #Add RTSP camera to VLM\n\u251c\u2500\u2500 view_rtsp.py  # View RTSP stream by opencv\n\u2514\u2500\u2500 vlm_voice.py  # multimodal main program\n")),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Start the VLM"),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/speech_vlm\nsudo docker compose up -d\n")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/dockerps.png"}))),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Add RTSP stream to VLM"))),(0,o.yg)("p",null,"View the contents of ",(0,o.yg)("inlineCode",{parentName:"p"},"set_streamer_id.sh")," under the ",(0,o.yg)("inlineCode",{parentName:"p"},"speech_vlm")," repository:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-sh"},"#!/bin/bash\ncurl --location 'http://0.0.0.0:5010/api/v1/live-stream' \\\n--header 'Content-Type: application/json' \\\n--data '{\"liveStreamUrl\": \"RTSP stream address\"}'\n")),(0,o.yg)("p",null,"Replace ",(0,o.yg)("inlineCode",{parentName:"p"},"0.0.0.0")," with the IP address of the Jetson device and replace ",(0,o.yg)("inlineCode",{parentName:"p"},"RTSP stream address")," with the camera's RTSP stream address.\nFor example:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-sh"},"#!/bin/bash\ncurl --location 'http://192.168.49.227:5010/api/v1/live-stream' \\\n--header 'Content-Type: application/json' \\\n--data '{\"liveStreamUrl\": \"rtsp://admin:IHFXnM8k@192.168.49.15:554//Streaming/Channels/1\"}'\n")),(0,o.yg)("admonition",{type:"note"},(0,o.yg)("p",{parentName:"admonition"},"If you don't have an RTSP camera, we have provided instructions on ",(0,o.yg)("a",{parentName:"p",href:"/getting_started_with_nvstreamer"},"how to use NVStreamer to stream local videos as RTSP")," and ",(0,o.yg)("a",{parentName:"p",href:"/run_vlm_on_recomputer"},"add them to the VLM"),".")),(0,o.yg)("p",null,"Run set_streamer_id.sh"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/speech_vlm\nsudo chmod +x ./set_streamer_id.sh\n./set_streamer_id.sh\n")),(0,o.yg)("p",null,"We will obtain a camera ID, this ID is very important and needs to be recorded, like this:"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/set_id.png"})),(0,o.yg)("ol",{start:3},(0,o.yg)("li",{parentName:"ol"},"Run vlm_voice.py")),(0,o.yg)("p",null,"You need to replace ",(0,o.yg)("inlineCode",{parentName:"p"},"0.0.0.0")," in the following two lines of Python code:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"API_URL = 'http://0.0.0.0:5010/api/v1/chat/completions'  # API endpoint\nREQUEST_ID = \"\"  # Request ID\n")),(0,o.yg)("p",null,"with the Jetson IP address and fill in the camera ID returned from Step 2 in place of ",(0,o.yg)("inlineCode",{parentName:"p"},"REQUEST_ID"),"."),(0,o.yg)("details",null,(0,o.yg)("summary",null,"vlm_voice.py"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import pyaudio\nimport wave\nimport keyboard\nimport subprocess\nimport json\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\nimport time\nimport torch\nfrom TTS.api import TTS\nimport os\n# Get device\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Init TTS\napi = TTS("tts_models/en/ljspeech/glow-tts").to(device)\n\n# Configuration parameters \nFORMAT = pyaudio.paInt16  # 16-bit resolution\nCHANNELS = 1  # Mono channel\nCHUNK = 1024  # Number of samples per chunk\nOUTPUT_FILENAME = "output.wav"  # Output file name\nAPI_URL = \'http://192.168.49.227:5010/api/v1/chat/completions\'  # API endpoint\nREQUEST_ID = "1388b691-3b9f-4bda-9d70-0ff0696f80f4"  # Request ID\n\n# Initialize PyAudio\naudio = pyaudio.PyAudio()\n# Prepare the list to store recording data\nframes = []\n\n# Initialize Micphone Rate\nprint("Available audio input devices:")\nfor i in range(audio.get_device_count()):\n    info = audio.get_device_info_by_index(i)\n    print(f"Device {i}: {info[\'name\']} - {info[\'maxInputChannels\']} channels")\n\ndevice_index = int(input("Please select the device index for your USB microphone: "))\n\ndevice_info = audio.get_device_info_by_index(device_index)\nsupported_sample_rates = [8000, 16000, 32000, 44100, 48000]\nsupported_rate=0\nfor rate in supported_sample_rates:\n    try:\n        if audio.is_format_supported(rate,\n                                     input_device=device_index,\n                                     input_channels=1,\n                                     input_format=pyaudio.paInt16):\n            supported_rate=rate\n            print(f"{rate} Hz is supported.")\n    except ValueError:\n        print(f"{rate} Hz is not supported.")\n\n\n# Initialize the model\nmodel = "./SenseVoiceSmall"\nmodel = AutoModel(\n    model=model,\n    vad_model="./speech_fsmn_vad_zh-cn-16k-common-pytorch",\n    vad_kwargs={"max_single_segment_time": 30000},\n    trust_remote_code=True,\n    disable_log=True\n)\n\n\n\ndef extract_content(json_response):\n    try:\n        # \u89e3\u6790JSON\u5b57\u7b26\u4e32\n        data = json.loads(json_response)\n        \n        # \u63d0\u53d6content\u90e8\u5206\n        content = data["choices"][0]["message"]["content"]\n        \n        print(f"{content}")\n        return content\n    except KeyError as e:\n        print(f"Key error: {e}")\n    except json.JSONDecodeError as e:\n        print(f"JSON decode error: {e}")\n    except Exception as e:\n        print(f"An unexpected error occurred: {e}")\n\ndef start_recording():\n    global frames\n    frames = []\n    \n    try:\n        stream = audio.open(format=FORMAT, channels=CHANNELS,\n                            rate=supported_rate, input=True,\n                            frames_per_buffer=CHUNK, input_device_index=device_index)\n        print("Recording started... Press \'2\' to stop recording.")\n    \n        while True:\n            if keyboard.is_pressed(\'2\'):\n                print("Recording stopped.")\n                break\n            data = stream.read(CHUNK)\n            frames.append(data)\n    \n        stream.stop_stream()\n        stream.close()\n    \n    except Exception as e:\n        print(f"An error occurred during recording: {e}")\n\ndef save_recording():\n    try:\n        waveFile = wave.open(OUTPUT_FILENAME, \'wb\')\n        waveFile.setnchannels(CHANNELS)\n        waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n        waveFile.setframerate(supported_rate)\n        waveFile.writeframes(b\'\'.join(frames))\n        waveFile.close()\n        print(f"Recording saved as {OUTPUT_FILENAME}")\n    except Exception as e:\n        print(f"An error occurred while saving the recording: {e}")\n\ndef send_alert(text):\n    # Construct the JSON payload\n    payload = {\n        "messages": [\n            {\n                "role": "system",\n                "content": "You are a helpful AI assistant."\n            },\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "stream",\n                        "stream": {\n                            "stream_id": REQUEST_ID\n                        }\n                    },\n                    {\n                        "type": "text",\n                        "text": text\n                    }\n                ]\n            }\n        ],\n        "min_tokens": 1,\n        "max_tokens": 128\n    }\n    \n    # Convert the payload to a JSON string\n    json_payload = json.dumps(payload)\n    \n    # Execute the curl command using subprocess\n    curl_command = [\n        \'curl\', \'--location\', API_URL,\n        \'--header\', \'Content-Type: application/json\',\n        \'--data\', json_payload\n    ]\n    \n    try:\n        result = subprocess.run(curl_command, check=True, capture_output=True, text=True)\n        ##Get words\n        content_result=extract_content(result.stdout)\n        # TTS \n        api.tts_to_file(\n            str(content_result),\n            speaker_wav="./example_1.wav",\n            file_path="speech.wav"\n        )\n        # Convert audio rate\n        subprocess.run([\'ffmpeg\', \'-i\', \'speech.wav\', \'-ar\',str(supported_rate), \'speech1.wav\',\'-y\'])\n        # Play audio\n        wf = wave.open(\'./speech1.wav\', \'rb\')\n        stream = audio.open(format=pyaudio.paInt16,\n                        channels=1,\n                        rate=supported_rate,\n                        output=True,\n                        output_device_index=device_index)\n        data = wf.readframes(1024)\n        while data:\n            stream.write(data)\n            data = wf.readframes(1024)\n        # Play audio\n        os.remove(\'speech.wav\')\n        os.remove(\'speech1.wav\')\n        stream.stop_stream()\n        stream.close()\n        wf.close()  # Close the wave file as well\n\n        #print(f"Alert sent successfully: {result.stdout}")\n    except subprocess.CalledProcessError as e:\n        print(f"An error occurred while sending the alert: {e.stderr}")\n    finally:\n        # Even if an error occurs, try to close the stream\n        if stream.is_active():\n            stream.stop_stream()\n            os.remove(\'speech.wav\')\n            os.remove(\'speech1.wav\')\n            stream.close()\nprint("Welcome to the Recording and Speech-to-Text System!")\nprint("Press \'1\' to start recording, \'2\' to stop recording.")\n\nwhile True:\n    if keyboard.is_pressed(\'1\'):\n        print("Preparing to start recording...")\n        start_recording()\n        save_recording()\n        \n        print("Processing the recording file, please wait...")\n        try:\n            res = model.generate(\n                input=f"./{OUTPUT_FILENAME}",\n                cache={},\n                language="auto",  # "zh", "en", "yue", "ja", "ko", "nospeech"\n                use_itn=True,\n                batch_size_s=60,\n                merge_vad=True,\n                merge_length_s=15,\n            )\n            text = rich_transcription_postprocess(res[0]["text"])\n            print(f"Speech-to-Text Result:\\n{text}")\n            \n            # Send the transcription result as an alert\n            send_alert(text)\n            \n        except Exception as e:\n            print(f"An error occurred while processing the recording: {e}")\n        \n    time.sleep(0.1)  # Reduce CPU usage\n'))),(0,o.yg)("p",null,"Run python:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"cd ~/speech_vlm\nsudo python3 vlm_voice.py\n")),(0,o.yg)("p",null,"After the program starts, it will scan all audio input and output devices. You will need to manually select the index ID of the desired audio device. The program is about to start working, then press ",(0,o.yg)("inlineCode",{parentName:"p"},"1")," to record and ",(0,o.yg)("inlineCode",{parentName:"p"},"2")," to send."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/select_mic.png"})),(0,o.yg)("ol",{start:4},(0,o.yg)("li",{parentName:"ol"},"View result")),(0,o.yg)("p",null,"We have prepared a ",(0,o.yg)("inlineCode",{parentName:"p"},"view_rtsp.py")," script to view the output results. You need to replace the IP part of the ",(0,o.yg)("inlineCode",{parentName:"p"},'rtsp_url = "rtsp://0.0.0.0:5011/out"')," with the IP address of your Jetson device."),(0,o.yg)("details",null,(0,o.yg)("summary",null,"viwe_rtsp.py"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import cv2\n\nrtsp_url = "rtsp://192.168.49.227:5011/out"\n\ncap = cv2.VideoCapture(rtsp_url)\n\nif not cap.isOpened():\n    print("Cannot open RTSP stream")\n    exit()\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        print("Failed to retrieve frame")\n        break\n\n    height, width = frame.shape[:2]\n\n    frame_resized = cv2.resize(frame, (int(width // 1.1), int(height // 1.1)))\n\n    cv2.imshow(\'RTSP Stream\', frame_resized)\n\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n'))),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"sudo pip3 install opencv-python\ncd ~/speech_vlm\nsudo python3 view_rtsp.py\n")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/Multimodal_ai/audio_vlm/view_result.png"})),(0,o.yg)("h3",{id:"demonstration"},"Demonstration"),(0,o.yg)("div",{align:"center"},(0,o.yg)("iframe",{width:"800",height:"450",src:"https://www.youtube.com/watch?v=cJRiCTPrCYk",title:"Multimodal Audio-Visual Large Language Model Demo",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0})),(0,o.yg)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,o.yg)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,o.yg)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,o.yg)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}u.isMDXComponent=!0}}]);